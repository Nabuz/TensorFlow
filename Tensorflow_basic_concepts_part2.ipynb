{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ----An example with tf.placeholder:----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[257.45944 254.60678 258.78598 ... 259.38147 258.58023 256.01477]\n",
      " [264.3697  257.96545 253.21733 ... 266.1487  257.69424 260.34152]\n",
      " [260.25763 250.60455 257.49536 ... 262.05325 257.29956 257.1919 ]\n",
      " ...\n",
      " [250.00182 252.39375 254.70703 ... 257.2407  251.31036 260.2009 ]\n",
      " [260.4658  255.94916 252.51193 ... 255.64287 256.9048  258.2929 ]\n",
      " [262.93042 254.96632 257.22543 ... 265.0838  260.0514  256.20804]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape =(1024,1024))\n",
    "y = tf.matmul(x,x)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    rand_array = np.random.rand(1024,1024)\n",
    "    print(sess.run(y, feed_dict={x:rand_array}))  #x tensor is feeded with the rand_array \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---Example with tensor.get_shape() and tensor.reshape()--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "shape tensor 1:  (2, 3)\n",
      "type:  <class 'tensorflow.python.framework.tensor_shape.TensorShape'>\n",
      "-------------\n",
      "shape tensor 2:  [6, 1]\n",
      "type:  <class 'list'>\n",
      "Number of column:  3\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "tensor = tf.convert_to_tensor(np.array([[1001,1002,1003],[3,4,5]]),dtype=tf.float32)\n",
    "\n",
    "sess.run(tensor)\n",
    "\n",
    "tensor_shape = tensor.get_shape()\n",
    "tensor_shape\n",
    "\n",
    "tensor2 = tf.reshape(tensor, tf.TensorShape([6,1]))\n",
    "\n",
    "print(\"------------\")\n",
    "print(\"shape tensor 1: \",tensor_shape)\n",
    "print(\"type: \",type(tensor_shape))\n",
    "print(\"-------------\")\n",
    "print(\"shape tensor 2: \", tensor2.get_shape().as_list())\n",
    "print(\"type: \",type(tensor.get_shape().as_list()))\n",
    "print(\"Number of column: \",int(tensor.get_shape()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### +++ Modularity: +++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to build a ReLU: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        if not hasattr(relu, \"threshold\"):\n",
    "            relu.threshold = tf.Variable(0.0, name =\"threshold\")\n",
    "        w_shape = (int(X.get_shape()[1]),1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape, name = \"weights\"))\n",
    "        X = tf.placeholder(tf.float32,shape=(None, n_features), name =\"X\") # a tensor with shape: (?,n_fatures)\n",
    "        z = tf.add(tf.matmul(X, w), relu.threshold, name =\"z\") #shape: (?,1) with ? number of rows for X variable\n",
    "        return tf.maximum(z, relu.threshold, name =\"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding 5 ReLu togheter: (and creating a graph in Tensorboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "n_features = 3\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\") #now = time in a specific format (YYYYMMDDHHMMSS)\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}\".format(root_logdir, now )\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name =\"X\")\n",
    "\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Perceptron algo for classify :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nabu-pc\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:,(2,3)] #petal length and petal witdth\n",
    "y = (iris.target == 0).astype(np.int)\n",
    "per_clf = Perceptron(random_state=42)\n",
    "per_clf.fit(X,y)\n",
    "\n",
    "y_pred = per_clf.predict([[2,0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a DNN : (Deep Neural Network):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We use the MNIST DB. It contains handwritten digits. <br> <br> Training set: 60 000 examples. Test set: 10 000 examples.<br> <br> It's a subset of a larger set named NIST. The digits have been size normalized and centered in a fixed-size image.  <br> <br> We have image of 28 X 28 pixel   with grey levels, not B/W image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --- Contruction Phase:---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28*28  #MNIST dataset.  . \n",
    "n_hidden1 = 300  #number of hidden neurons for layer 1 and 2 \n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\") #now = time in a specific format (YYYYMMDDHHMMSS)\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}\".format(root_logdir, now ) # The path where we save the data: tf_logs/run-YYYYMMDDHHMMSS\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs),name=\"X\") #--The TRAINING BATCH (X)-- shape: (None, n_inputs)\n",
    "                                                                # X acts as \"input layer\". \n",
    "                                                                #It will be replaced with one training batch at the time\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\") # --y-- shape : (None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron layer function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have different parameters: inputs, number of neurons, the activation function and the name of the layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_layer(X,n_neurons, name, activation = None):\n",
    "    with tf.name_scope(name):  #name_scope is useful in Tensorboard\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs + n_neurons)\n",
    "        init = tf.truncated_normal((n_inputs,n_neurons),stddev = stddev)\n",
    "        W = tf.Variable(init, name =\"kernel\") # The kernel layer is a matrix (2d tensor )that holds the WEIGHTS\n",
    "                                              # It's contains all connection weights between each input and each neuron\n",
    "                                              # Its shape: (n_inputs,n_neurons)\n",
    "                                              # it will be initialized randomly, using a truncated normal distribution with stdev\n",
    "                                              # With this specific stddev the algorithm converge faster\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\") #For biases. Initialized to 0. One bias parameter per neuron\n",
    "        Z = tf.matmul(X,W) + b # Shape : (None, n_neurons)\n",
    "                               # This vectorized implementation will efficiently compute the weight sums of the inputs plus bias  \n",
    "                               # for each neuron in the layer, for all instances in the batch in one shot.\n",
    "        if activation is not None:\n",
    "            return activation(Z)     # if an activation parameter is provided, such as tf.nn.relu (i.e. max(0,Z)), the function \n",
    "        else:                        # returns the activation(Z), else returns Z\n",
    "            return Z\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating different layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = neuron_layer(X,n_hidden1,name=\"hidden1\", activation=tf.nn.relu)\n",
    "    \n",
    "    hidden2 = neuron_layer(hidden1,n_hidden2, name = \"hidden2\", activation=tf.nn.relu)\n",
    "    \n",
    "    logits = neuron_layer(hidden2, n_outputs, name=\"outputs\") # the output of the neural networks before the activation \n",
    "                                                              # function softmax. For optimization reasons softmax will be compute later\n",
    "    \n",
    "  # We can also create layer without neuron_layer() function. Tensorflow include method to create fully connected layers\n",
    "  # where all inputs are connected to all neurons in the layer\n",
    "    \n",
    "  # hidden1 = tf.layers.dense(X,n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "  # hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", activation= tf.nn.relu)\n",
    "  # logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Cost function (Cross Entropy)/ Gradient Optimizer / Accuracy: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We define the Cross Entropy as the Cost function that we use to train the nn. <br> <br> Weak aspect :  this cross function will penalize models that estimate a low probability for the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits = logits)\n",
    "    # It calculates the cross entropy based on the \"logits\" (the outpute before going through the softmax activation function)\n",
    "    # It expects labels in the form of integers ranging from 0 to the number of classes minus 1 (0-9)\n",
    "    #The output is 1D tensor with the cross entropy for each istance.\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\") #it computes the mean cross entropy over all instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As optimizer we use the Gradient Descent in order to minimize the Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We calculate the accuracy of our our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y,1) #For each instance it determines if the nn's prediction is correct by checking wheter \n",
    "                                          #or not the highest logit corresponds to the target class\n",
    "                                          #It returns a 1d Tensor full of boolean values\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing / save phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer() #We create a node to initialize all variables\n",
    "saver = tf.train.Saver() # We will create a Saver to save our trained model parameters to disk\n",
    "\n",
    "loss_summary = tf.summary.scalar('LOSS', loss) # Creates a node in the graph that will evaluate the MSE value and writes it to a TensoBoard-compatible binary log string called Summary\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph()) # We create a filewriter that you use to write summaries to logfiles in the log directory\n",
    "                                                                    # logdir = the patph of the logdir diretory. Second parameter = the graph we want to visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---Execution Phase:---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We import data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from  tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\") # the minst object has different method. Some methods can extract \n",
    "                                                             # the training istance (50k instance), a validation set(5k instances)\n",
    "                                                             # a test set(10k)\n",
    "n_epochs = 40  # numbers of eboch we want to run\n",
    "batch_size = 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy 0.92 Test accuracy 0.9124\n",
      "1 Train accuracy 0.96 Test accuracy 0.9292\n",
      "2 Train accuracy 0.96 Test accuracy 0.9372\n",
      "3 Train accuracy 1.0 Test accuracy 0.9448\n",
      "4 Train accuracy 0.94 Test accuracy 0.95\n",
      "5 Train accuracy 0.92 Test accuracy 0.956\n",
      "6 Train accuracy 0.96 Test accuracy 0.9604\n",
      "7 Train accuracy 0.94 Test accuracy 0.9618\n",
      "8 Train accuracy 0.94 Test accuracy 0.964\n",
      "9 Train accuracy 0.92 Test accuracy 0.9638\n",
      "10 Train accuracy 0.96 Test accuracy 0.9674\n",
      "11 Train accuracy 1.0 Test accuracy 0.967\n",
      "12 Train accuracy 0.96 Test accuracy 0.9674\n",
      "13 Train accuracy 0.98 Test accuracy 0.9698\n",
      "14 Train accuracy 1.0 Test accuracy 0.9698\n",
      "15 Train accuracy 0.98 Test accuracy 0.9712\n",
      "16 Train accuracy 0.96 Test accuracy 0.9718\n",
      "17 Train accuracy 0.96 Test accuracy 0.973\n",
      "18 Train accuracy 0.96 Test accuracy 0.973\n",
      "19 Train accuracy 0.98 Test accuracy 0.974\n",
      "20 Train accuracy 1.0 Test accuracy 0.9724\n",
      "21 Train accuracy 1.0 Test accuracy 0.9722\n",
      "22 Train accuracy 1.0 Test accuracy 0.973\n",
      "23 Train accuracy 0.98 Test accuracy 0.9732\n",
      "24 Train accuracy 0.96 Test accuracy 0.9744\n",
      "25 Train accuracy 0.98 Test accuracy 0.9742\n",
      "26 Train accuracy 1.0 Test accuracy 0.974\n",
      "27 Train accuracy 1.0 Test accuracy 0.975\n",
      "28 Train accuracy 1.0 Test accuracy 0.9754\n",
      "29 Train accuracy 0.98 Test accuracy 0.9752\n",
      "30 Train accuracy 1.0 Test accuracy 0.9758\n",
      "31 Train accuracy 0.98 Test accuracy 0.9754\n",
      "32 Train accuracy 1.0 Test accuracy 0.9764\n",
      "33 Train accuracy 1.0 Test accuracy 0.9748\n",
      "34 Train accuracy 1.0 Test accuracy 0.975\n",
      "35 Train accuracy 1.0 Test accuracy 0.976\n",
      "36 Train accuracy 1.0 Test accuracy 0.9754\n",
      "37 Train accuracy 1.0 Test accuracy 0.9758\n",
      "38 Train accuracy 0.98 Test accuracy 0.9756\n",
      "39 Train accuracy 1.0 Test accuracy 0.9762\n"
     ]
    }
   ],
   "source": [
    "n_batches = mnist.train.num_examples // batch_size\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()  #an initial node that initializes all the variables\n",
    "    for epoch in range(n_epochs): \n",
    "        for iteration in range(mnist.train.num_examples // batch_size):  # // operator returns returns int(a/b)\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)      # for each epoch the algo uses all batchs in the dataset (the whole training size)\n",
    "            summary_str = loss_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            step = epoch * n_batches + iteration  \n",
    "            file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch}) \n",
    "        acc_train = accuracy.eval(feed_dict = {X: X_batch, y: y_batch})#Here X_bath and y_batch come from last bath in Training Set\n",
    "        acc_val = accuracy.eval(feed_dict = {X: mnist.validation.images, y: mnist.validation.labels}) #the algo is evaluated on \n",
    "                                                                                                      #the full validation set\n",
    "        # For TensorBoard purpose is better to calculate the accuracy only on acc_train!!!! \n",
    "        \n",
    "        print(epoch, \"Train accuracy\", acc_train, \"Test accuracy\", acc_val) \n",
    "        \n",
    "        \n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final2.ckpt\")\n",
    "    file_writer.close()\n",
    "    \n",
    "# For running Tensorboard, in the Anaconda Prompt IN /Progetto Forex/Python (our working directory) -> tensorboard --logdir tf_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---Using the Neural Network:---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the nn is trained. we can use it to make predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(mnist.test.images)   #We need to scaling the images. Each images has 28*28 pixel -> 784 pixels.Each pixel a range(-1,1)\n",
    "images_scaled =scaler.transform(mnist.test.images)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final2.ckpt\") #We load the model parameters from the disk\n",
    "    Z = logits.eval(feed_dict={X: images_scaled}) #We evaluate the logits node\n",
    "    y_pred = np.argmax(Z,axis =1) # For predicting a class we pick the class that has the highest logit value (argmax())\n",
    "    \n",
    "    # ---IMP!!!---if we want to know all estimates class probabilities we need to apply the the softmax() function to the logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing the first 30 predictions of Test Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 5 4 7 0 6 9 0 1 5 7 7 3 4 7 6 6 5 4 0 7 4 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred[:30])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "244px",
    "left": "1727.25px",
    "right": "20px",
    "top": "125px",
    "width": "400px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
